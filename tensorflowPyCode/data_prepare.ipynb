{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_prepare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUvZMxfOrnAW",
        "colab_type": "code",
        "outputId": "188a110b-811b-4192-a411-2648cc083331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnwBgKZqD3sM",
        "colab_type": "code",
        "outputId": "f59c1a50-a877-4974-aa3c-c3f964b3b709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import zeros\n",
        "from numpy import array\n",
        "from numpy.random import choice\n",
        "\n",
        "#nltk.download('punkt')\n",
        "\n",
        "def merge_entity_metadata_by_id():\n",
        "  entity_ids_df = pd.read_csv(os.path.join(DATA_DIR, \"entity_ids.csv\"), \n",
        "                              usecols=['entity', 'id'],\n",
        "                              index_col='id',\n",
        "                             )\n",
        "  #print(entity_ids_df)\n",
        "  \n",
        "  entity_metadata_df = pd.read_csv(os.path.join(DATA_DIR, \"entity_metadata.csv\"),\n",
        "                                  )\n",
        "  entity_metadata_df = entity_metadata_df.rename(columns={'entity':'id', 'humanreadablererpresentation': 'metadata'})\n",
        "  #print(entity_metadata_df)\n",
        "  \n",
        "  # merge data \n",
        "  id_entity_metadata_df = pd.merge(entity_ids_df,entity_metadata_df, on=['id'], how= 'left')\n",
        "#   print(id_entity_metadata_df.head(10))\n",
        "  id_entity_metadata_df = id_entity_metadata_df.set_index('entity').filter(regex=\"[<][a-zA-Z]+[>]\", axis=0)\n",
        "#   id_entity_metadata_df = id_entity_metadata_df.set_index('entity').filter(regex=\"[<][a-z]+[>]\", axis=0)\n",
        "  return id_entity_metadata_df\n",
        "  \n",
        "def merge_entity_metatype_by_id():\n",
        "  entity_types_df = pd.read_csv(os.path.join(DATA_DIR, \"entity_types.csv\"),\n",
        "                               )\n",
        "  entity_types_df = entity_types_df.rename(columns={'entity':'id'})\n",
        "  #print(entity_types_df)\n",
        "  \n",
        "  type_ids_df = pd.read_csv(os.path.join(DATA_DIR, \"type_ids.csv\"),\n",
        "                            usecols=['id', 'metatype'],\n",
        "                           )\n",
        "  type_ids_df = type_ids_df.rename(columns={'id':'type'})\n",
        "  #print(type_ids_df)\n",
        "  \n",
        "  id_type_metatype_df = pd.merge(entity_types_df,type_ids_df, on=['type'], how= 'left')\n",
        "  return id_type_metatype_df\n",
        "\n",
        "def merge_news_header_fulltext():\n",
        "  # get_text_id_header_df\n",
        "  id_header_file = os.path.join(DATA_DIR, \"full-news-10000.txt\")\n",
        "  f = open(id_header_file)\n",
        "  lines = f.readlines()\n",
        "  f.close\n",
        "  id_header_dict = {}\n",
        "  for line in lines:\n",
        "    text_id = line.split(\",\")[0]\n",
        "    header = line.split(',,\"')[-1]\n",
        "    if text_id not in id_header_dict:\n",
        "      header = header.rstrip('\"\\n')\n",
        "      id_header_dict[text_id] = header\n",
        "#   print(len(id_header_dict))\n",
        "  id_header_df = pd.DataFrame(list(id_header_dict.items()), columns=['id', 'header'])\n",
        "#   print(id_header_df.head(10))\n",
        "  \n",
        "  # get_id_fulltext_df\n",
        "  id_fulltext_file = os.path.join(DATA_DIR, \"text.txt\")\n",
        "  f = open(id_fulltext_file)\n",
        "  lines = f.readlines()\n",
        "  f.close()\n",
        "  id_fulltext_dict = {}\n",
        "  for index, line in enumerate(lines):\n",
        "    if index % 2 == 0:\n",
        "      text_id = line.rstrip(\"\\n\")\n",
        "    elif text_id not in id_fulltext_dict:\n",
        "      id_fulltext_dict[text_id] = line\n",
        "#   print(len(id_fulltext_dict))\n",
        "  id_fulltext_df = pd.DataFrame(list(id_fulltext_dict.items()), columns=['id', 'fulltext'])\n",
        "#   print(id_fulltext_df.head(10))\n",
        "  \n",
        "  #merge_header_fulltext_by_id\n",
        "  id_header_fulltext_df = pd.merge(id_fulltext_df,id_header_df, on=['id'], how= 'left')\n",
        "  return id_header_fulltext_df\n",
        "  \n",
        "# text to list of tokenized sents\n",
        "def text_to_sents(text_list):\n",
        "  sents = []\n",
        "  for text in text_list:\n",
        "    # sentence split\n",
        "    sent_text = nltk.sent_tokenize(text)\n",
        "    for sentence in sent_text:\n",
        "      # word tokenization\n",
        "      tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+\\'?\\w*')\n",
        "      tokenized_sent = tokenizer.tokenize(sentence)\n",
        "      tokenized_sent = [word.lower() for word in tokenized_sent]\n",
        "      sents.append(tokenized_sent)\n",
        "  return sents\n",
        "\n",
        "# word frequency \n",
        "def sents_processing(sents):\n",
        "  # build new dict to count words in poems\n",
        "  allWords = {} \n",
        "  for sent in sents:\n",
        "    for word in sent:\n",
        "      if word not in allWords:\n",
        "        #print(word)\n",
        "        allWords[word] = 1\n",
        "      else:\n",
        "        allWords[word] = allWords[word]+ 1\n",
        "  # delete words which less than 2 times used in poems\n",
        "  erase = []\n",
        "  for key in allWords:\n",
        "    if allWords[key] < 5:\n",
        "        erase.append(key)\n",
        "  #print(erase)\n",
        "  allWords['<oov/>'] = 0\n",
        "  for key in erase:\n",
        "    allWords['<oov/>'] = allWords['<oov/>']+ allWords[key]\n",
        "    del allWords[key]\n",
        "  # sort allWords by counting times: key:word, value:count\n",
        "  wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
        "  print(wordPairs)\n",
        "  words, a= zip(*wordPairs)\n",
        "  # word to ID, most frequently used word is assigned with a short id\n",
        "  word2int = dict(zip(words, range(len(words)))) \n",
        "#   print(word2int)\n",
        "  # ID to word\n",
        "  int2word = dict(zip(word2int.values(), word2int.keys()))\n",
        "  #print(reverse_dictionary)\n",
        "  return allWords, word2int, int2word\n",
        "\n",
        "id_entity_metadata_df = merge_entity_metadata_by_id()\n",
        "# print(id_entity_metadata_df)\n",
        "entity_list = id_entity_metadata_df['metadata'].tolist()\n",
        "# print(len(entity_list))\n",
        "# print(entity_list[:10])\n",
        "chars_list = word_to_chars_list(entity_list)\n",
        "print(len(chars_list)) # 358947\n",
        "print(chars_list[:10])\n",
        "\n",
        "chars_dict, char2int, int2char = chars_list_processing(chars_list)\n",
        "print(char2int)\n",
        "print(len(char2int))\n",
        "\n",
        "# print(chr(96))\n",
        "\n",
        "# id_type_metatype_df = merge_entity_metatype_by_id()\n",
        "# print(id_type_metatype_df.head(5))\n",
        "\n",
        "# id_entity_metadata_metatype_df = pd.merge(id_type_metatype_df, id_entity_metadata_df, on=['id'], how='left')\n",
        "# print(id_entity_metadata_metatype_df.head(5))\n",
        "# id_header_fulltext_df = merge_news_header_fulltext()\n",
        "# print(id_header_fulltext_df.head(10))\n",
        "\n",
        "# header_list = np.array(id_header_fulltext_df['header']).tolist()\n",
        "# fulltext_list = np.array(id_header_fulltext_df['fulltext']).tolist()\n",
        "# print(len(header_list))\n",
        "# print(header_list[:10])\n",
        "\n",
        "# all_text_list = header_list + fulltext_list\n",
        "# sents = text_to_sents(all_text_list)\n",
        "\n",
        "# print(len(sents))\n",
        "# print(sents[:10])\n",
        "\n",
        "# allWords, word2int, int2word = sents_processing(sents)\n",
        "# print(len(word2int))\n",
        "# print(word2int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "358947\n",
            "[['A', 'a', 'a', 'a', 'b', 'a', '</END>'], ['A', 'a', 'a', 'g', 'e', 's', '</END>'], ['A', 'a', 'a', 'j', 'i', 'a', 'o', '</END>'], ['A', 'a', 'a', 'r', 't', 'a', 'l', 'i', '</END>'], ['A', 'a', 'b', 'a', '</END>'], ['A', 'a', 'b', 'a', 'v', 'a', 'n', 'a', 'n', '</END>'], ['A', 'A', 'B', 'B', '</END>'], ['A', 'a', 'b', 'e', 'n', 'r', 'a', 'a', '</END>'], ['A', 'a', 'b', 'e', 'y', '</END>'], ['A', 'a', 'b', 'h', 'a', 'r', 'a', 'n', 'a', 'c', 'h', 'a', 'r', 't', 'h', 'u', '</END>']]\n",
            "[('a', 361648), ('</END>', 358947), ('e', 245987), ('i', 219692), ('o', 198300), ('r', 187215), ('n', 185985), ('l', 142470), ('t', 129158), ('s', 126723), ('u', 113323), ('h', 99443), ('d', 77411), ('m', 72188), ('c', 71620), ('g', 57729), ('p', 51213), ('k', 51152), ('y', 49626), ('b', 41598), ('S', 37637), ('A', 31119), ('M', 30354), ('C', 29051), ('K', 28309), ('B', 28138), ('P', 27683), ('v', 27039), ('T', 24469), ('z', 22914), ('w', 21071), ('D', 18782), ('G', 17097), ('L', 17093), ('f', 16990), ('R', 16209), ('N', 15455), ('W', 15439), ('H', 15327), ('E', 13768), ('j', 12418), ('O', 11359), ('V', 11154), ('F', 10880), ('I', 10233), ('x', 8762), ('J', 6736), ('Z', 5739), ('U', 5624), ('q', 4313), ('Y', 4233), ('X', 3207), ('Q', 3193)]\n",
            "{'a': 0, '</END>': 1, 'e': 2, 'i': 3, 'o': 4, 'r': 5, 'n': 6, 'l': 7, 't': 8, 's': 9, 'u': 10, 'h': 11, 'd': 12, 'm': 13, 'c': 14, 'g': 15, 'p': 16, 'k': 17, 'y': 18, 'b': 19, 'S': 20, 'A': 21, 'M': 22, 'C': 23, 'K': 24, 'B': 25, 'P': 26, 'v': 27, 'T': 28, 'z': 29, 'w': 30, 'D': 31, 'G': 32, 'L': 33, 'f': 34, 'R': 35, 'N': 36, 'W': 37, 'H': 38, 'E': 39, 'j': 40, 'O': 41, 'V': 42, 'F': 43, 'I': 44, 'x': 45, 'J': 46, 'Z': 47, 'U': 48, 'q': 49, 'Y': 50, 'X': 51, 'Q': 52}\n",
            "{'a': 0, '</END>': 1, 'e': 2, 'i': 3, 'o': 4, 'r': 5, 'n': 6, 'l': 7, 't': 8, 's': 9, 'u': 10, 'h': 11, 'd': 12, 'm': 13, 'c': 14, 'g': 15, 'p': 16, 'k': 17, 'y': 18, 'b': 19, 'S': 20, 'A': 21, 'M': 22, 'C': 23, 'K': 24, 'B': 25, 'P': 26, 'v': 27, 'T': 28, 'z': 29, 'w': 30, 'D': 31, 'G': 32, 'L': 33, 'f': 34, 'R': 35, 'N': 36, 'W': 37, 'H': 38, 'E': 39, 'j': 40, 'O': 41, 'V': 42, 'F': 43, 'I': 44, 'x': 45, 'J': 46, 'Z': 47, 'U': 48, 'q': 49, 'Y': 50, 'X': 51, 'Q': 52}\n",
            "53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrFL2T9DsK6l",
        "colab_type": "code",
        "outputId": "ae2cee56-b970-4252-f4e2-f45abf468195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, model_from_json, load_model\n",
        "from keras.layers import Dense, Embedding, Activation\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "########################################## Params ##########################################\n",
        "\n",
        "MAX_LEN = 3\n",
        "VOCAB_SIZE = 53\n",
        "# totol news count 10000\n",
        "START_INDEX = 0 # start index\n",
        "END_INDEX = 10 # trainig number\n",
        "\n",
        "# model building\n",
        "INIT_LR = 0.001 # init learning rate\n",
        "N_STEP = 10 \n",
        "BATCH_SIZE = 16\n",
        "LAYERS_DIM = [700, 700]\n",
        "EPOCHS = 15 #training_epochs number\n",
        "\n",
        "########################################## Embed Paths ##########################################\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "EMBED_DIR = \"drive/My Drive/DATA/\" # embedding file\n",
        "glove_embedding = \"glove.6B.{:d}d.txt\".format(EMBED_DIM)\n",
        "\n",
        "########################################## Data Prepare ##########################################\n",
        "\n",
        "def char_encoding(chars_list):\n",
        "  chars_integer = []\n",
        "  for chars in chars_list:\n",
        "    char_integer = []\n",
        "    for char in chars:\n",
        "      char_integer.append(char2int[char])\n",
        "    chars_integer.append(char_integer)\n",
        "  return chars_integer\n",
        "\n",
        "def N_gram_processing(chars, N):\n",
        "  N_gram_data=[]\n",
        "  N_gram_label=[]\n",
        "  for i in range(len(chars)-N):\n",
        "    N_gram = chars[i:i+N+1]\n",
        "    N_gram_data.append(tuple(N_gram[:-1]))\n",
        "    N_gram_label.append(N_gram[-1])\n",
        "  return N_gram_data, N_gram_label\n",
        "\n",
        "def get_train_data(chars_list, MAX_LEN):\n",
        "  print(chars_list[:1])\n",
        "  chars_list = char_encoding(chars_list)\n",
        "  print(chars_list[:1])\n",
        "  datas = []\n",
        "  labels = []\n",
        "  for chars in chars_list:\n",
        "    data, label=N_gram_processing(chars, MAX_LEN)\n",
        "    datas = datas+list(data)\n",
        "    labels = labels+label\n",
        "  X_data = np.empty((len(labels), MAX_LEN), dtype = int)\n",
        "  y_label = np.empty((len(labels),VOCAB_SIZE), dtype = int)\n",
        "  for index, _ in enumerate(labels):\n",
        "    X_data[index, : ] = [int(x) for x in list(datas[index])]\n",
        "    y_label[index] = to_categorical(labels[index], VOCAB_SIZE)\n",
        "  #del datas, labels\n",
        "  return X_data, y_label\n",
        "\n",
        "def get_embedding_matrix(dictionary):\n",
        "  embedding_matrix = zeros((VOCAB_SIZE, EMBEDDING_DIM), dtype=float)\n",
        "  for i, key in enumerate(dictionary.keys()):\n",
        "    embedding_vector = [float(w) for w in np.random.rand(1, EMBEDDING_DIM)[0]]\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "\n",
        "\n",
        "def lr_scheduler(epoch):\n",
        "  if epoch == 0:\n",
        "    K.set_value(model.optimizer.lr,INIT_LR)\n",
        "    print(\"start lr set to {}\".format(K.get_value(model.optimizer.lr)))\n",
        "  # every 10 step reduce lr to 0.1*lr\n",
        "  elif epoch % N_STEP == 0 and epoch != 0:\n",
        "    lr = K.get_value(model.optimizer.lr)\n",
        "    K.set_value(model.optimizer.lr, lr * 0.1)\n",
        "    print(\"lr changed to {}\".format(K.get_value(model.optimizer.lr)))\n",
        "  return K.get_value(model.optimizer.lr)\n",
        "\n",
        "train_data, train_label = get_train_data(chars_list[:END_INDEX], MAX_LEN)\n",
        "print(train_data[:10])\n",
        "print(train_label[:10])\n",
        "\n",
        "embedding_matrix = get_embedding_matrix(char2int)\n",
        "print(embedding_matrix)\n",
        "\n",
        "print(\"finished data preparation !!! \")\n",
        "\n",
        "model = Sequential()\n",
        "# input Layer\n",
        "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=True))\n",
        "# Hidden Layer\n",
        "# model.add(LSTM(Layer1_dim, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(LSTM(LAYERS_DIM[0], return_sequences = True, dropout = 0.2, recurrent_dropout=0.2))\n",
        "model.add(Activation('relu')) \n",
        "# model.add(LSTM(LAYERS_DIM[0], return_sequences = True, dropout = 0.3, recurrent_dropout=0.3))\n",
        "# model.add(Activation('relu'))\n",
        "model.add(LSTM(LAYERS_DIM[1], dropout= 0.2, activation='relu'))\n",
        "# Output Layer\n",
        "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "\n",
        "# fit the model\n",
        "lr_reduce = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "# mc = ModelCheckpoint('hex06_best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(train_data, train_label, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[lr_reduce], \n",
        "#           validation_data=(valid_data, valid_label)\n",
        "         )\n",
        "\n",
        "# loss, accuracy = model.evaluate(valid_data, valid_label, batch_size=valid_num)\n",
        "# print('Accuracy of Valid_data: %f' % (accuracy*100))\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(model2json_path, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(weights_path)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['A', 'a', 'a', 'a', 'b', 'a', '</END>']]\n",
            "[[21, 0, 0, 0, 19, 0, 1]]\n",
            "[[21  0  0]\n",
            " [ 0  0  0]\n",
            " [ 0  0 19]\n",
            " [ 0 19  0]\n",
            " [21  0  0]\n",
            " [ 0  0 15]\n",
            " [ 0 15  2]\n",
            " [15  2  9]\n",
            " [21  0  0]\n",
            " [ 0  0 40]]\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[[0.44020734 0.54489157 0.09992419 ... 0.10174837 0.80264666 0.7754158 ]\n",
            " [0.17220321 0.52316379 0.45265868 ... 0.40399555 0.50151059 0.63680757]\n",
            " [0.1861777  0.64158239 0.90330587 ... 0.16126568 0.94691971 0.29378955]\n",
            " ...\n",
            " [0.9919802  0.28184704 0.05324053 ... 0.56982997 0.50995685 0.4683602 ]\n",
            " [0.95745007 0.6851746  0.07290335 ... 0.06565202 0.7228648  0.2685737 ]\n",
            " [0.93814075 0.84895822 0.2124756  ... 0.89448821 0.35206376 0.21033135]]\n",
            "finished data preparation !!! \n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 3, 100)            5300      \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 3, 700)            2242800   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 700)            0         \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 700)               3922800   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 53)                37153     \n",
            "=================================================================\n",
            "Total params: 6,208,053\n",
            "Trainable params: 6,208,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "start lr set to 0.0010000000474974513\n",
            "53/53 [==============================] - 6s 108ms/step - loss: 3.8956 - acc: 0.1698\n",
            "Epoch 2/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 3.3744 - acc: 0.2642\n",
            "Epoch 3/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.8913 - acc: 0.1698\n",
            "Epoch 4/15\n",
            "53/53 [==============================] - 1s 24ms/step - loss: 2.8818 - acc: 0.1887\n",
            "Epoch 5/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.6629 - acc: 0.1698\n",
            "Epoch 6/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.6618 - acc: 0.2075\n",
            "Epoch 7/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.5723 - acc: 0.2642\n",
            "Epoch 8/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.5253 - acc: 0.2075\n",
            "Epoch 9/15\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 2.4930 - acc: 0.2830\n",
            "Epoch 10/15\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 2.4814 - acc: 0.2642\n",
            "Epoch 11/15\n",
            "lr changed to 0.00010000000474974513\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 2.4698 - acc: 0.2642\n",
            "Epoch 12/15\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 2.4653 - acc: 0.2642\n",
            "Epoch 13/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.4613 - acc: 0.2642\n",
            "Epoch 14/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.4421 - acc: 0.2642\n",
            "Epoch 15/15\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 2.4403 - acc: 0.2642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-1eed555bb9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# serialize model to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2json_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model2json_path' is not defined"
          ]
        }
      ]
    }
  ]
}